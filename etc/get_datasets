#!/bin/bash

# check command line
if [ -z "$1" ]; then
    echo "Usage: get_datasets <dir> <dataset_file>"
    exit 1
fi

if [ -z "$2" ]; then
    echo "Usage: get_datasets <dir> <dataset_file>"
    exit 1
fi

if [ ! -e "$1" ]; then
    mkdir -p $1
fi

if [ ! -d "$1" ]; then
    echo "Not a directory: $1"
    exit 1
fi

if [ ! -f "$2" ]; then
    echo "Not a file: $2"
    exit 1
fi

OUT=${1%/}

source "$2"

OUT_SELF=$OUT/$OUT_DIR
mkdir -p $OUT_SELF

OUT_TMP=$OUT/tmp
mkdir -p $OUT_TMP

function gen_prefix {
    F=$1
    if [ -f $F ]; then
        F_MB=$((`stat -c %s $F`/1024/1024))

        for S in $PREFIX_SIZES; do
            if [ $F_MB -ge $S ]; then
                FP=${F}.${S}MB
                if [ ! -f $FP ]; then
                    echo "$F -> $FP"
                    cat $F | dd count=${S}K bs=1K > $FP
                fi
            fi
        done
    fi
}

function abs_path {
    echo "$(cd "$(dirname "$1")"; pwd)/$(basename "$1")"
}

# Plain or compressed file sources
for T in ${URLS[@]}; do
    # filename of download file
    F_TMP=$OUT_TMP/`basename $T`

    # filename for final dataset file
    # removes .gz endings for files that need to be decompressed
    if [[ $F_TMP == *.gz ]]; then
        F=$OUT_SELF/`basename $T .gz`
    else
        F=$OUT_SELF/`basename $T`
    fi

    # download base file if necessary
    if [ ! -f $F ]; then
        # make sure we don't get partial downloads by preventing the
        # script from executing further
        set -e

        wget -c -P $OUT_TMP $T

        # decompress .gz files
        if [[ $F_TMP == *.gz ]]; then
            gzip -d $F_TMP
            F_TMP=$OUT_TMP/`basename $T .gz`
        fi

        mv $F_TMP $F
        set +e
    fi

    # generate prefixes
    gen_prefix $F

done

# Wikipedia sources
WIKI_EXTRACT=./WikiExtractor.py

WIKI_TIME="2016-01-01T00:00:00Z"
WIKI_ARTS_DIR=wikipedia

# How many articles to fetch at once
WIKI_CHUNK_SIZE=100

function wiki_export {
    WIKI_ARTS_HEX=`cat $2 | hexdump -v -e '/1 "%02x"' | sed 's/\(..\)/%\1/g'`

    curl -d "" "https://$1.wikipedia.org/wiki/Special:Export?pages=$WIKI_ARTS_HEX&offset=$WIKI_TIME&limit=1" > $3
}

for((n=0;n<${#WIKI[@]};n+=2)); do
    L=${WIKI[$n]}
    L_FILE=${WIKI[(($n + 1))]}
    echo "lang: $L, file: $L_FILE"
    F=$OUT_SELF/$L_FILE
    F_TMP=$OUT_TMP/$L_FILE

    WIKI_ARTS_FILE=$WIKI_ARTS_DIR/$L_FILE

    if [ ! -f $F ]; then
        # Split up list of articles into a number of files with fixed amount of lines
        split -d -l $WIKI_CHUNK_SIZE -a 3 $WIKI_ARTS_FILE "$OUT_TMP/$L_FILE"".split"
        for split_file in "$OUT_TMP/$L_FILE"".split"*; do
            wiki_export $L $split_file $split_file.xml
            rm $split_file

            $WIKI_EXTRACT --no-templates --no-doc --no-title -o - $split_file.xml > $split_file.exported
            rm $split_file.xml
        done

        # Concatenate all articles
        cat "$OUT_TMP/$L_FILE"".split"*".exported" > "$OUT_TMP/$L_FILE"
        rm "$OUT_TMP/$L_FILE"".split"*".exported"
        mv "$OUT_TMP/$L_FILE" $F
    fi

    # generate prefixes
    gen_prefix $F
done
